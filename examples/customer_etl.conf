# Example batch ETL pipeline configuration.
#
# Reads raw customers, transforms with SQL, and writes to a curated table.
#
# Usage:
#   from pyspark_pipeline_framework.runner import SimplePipelineRunner
#   runner = SimplePipelineRunner.from_file("examples/customer_etl.conf")
#   result = runner.run()

{
  name: "customer-etl"
  version: "1.0.0"
  environment: dev
  mode: batch

  spark {
    app_name: "Customer ETL Pipeline"
    master: "local[*]"
    spark_conf {
      "spark.sql.shuffle.partitions": "10"
    }
  }

  components: [
    {
      name: "read_raw_customers"
      component_type: source
      class_path: "pyspark_pipeline_framework.examples.batch.ReadTable"
      config {
        table_name: "raw.customers"
        output_view: "raw_customers"
        filter_condition: "created_at >= '2024-01-01'"
      }
    },
    {
      name: "transform_customers"
      component_type: transformation
      class_path: "pyspark_pipeline_framework.examples.batch.SqlTransform"
      depends_on: ["read_raw_customers"]
      config {
        sql: """
          SELECT
            customer_id,
            UPPER(name) AS name,
            LOWER(email) AS email,
            created_at
          FROM raw_customers
          WHERE email IS NOT NULL
        """
        output_view: "cleaned_customers"
      }
    },
    {
      name: "write_customers"
      component_type: sink
      class_path: "pyspark_pipeline_framework.examples.batch.WriteTable"
      depends_on: ["transform_customers"]
      config {
        input_view: "cleaned_customers"
        output_table: "curated.customers"
        mode: "overwrite"
      }
    }
  ]
}
