# Example HOCON configuration for pyspark-pipeline-framework
# This demonstrates all configuration features

name = "sample-etl-pipeline"
version = "1.0.0"
environment = "dev"
mode = "batch"

# Spark runtime configuration
spark {
  app_name = "Sample ETL Pipeline"
  master = "local[*]"
  deploy_mode = "client"
  driver_memory = "2g"
  driver_cores = 1
  executor_memory = "4g"
  executor_cores = 2
  num_executors = 2
  dynamic_allocation = false

  # Additional Spark configuration
  spark_conf {
    "spark.sql.shuffle.partitions" = "200"
    "spark.sql.adaptive.enabled" = "true"
    "spark.executor.memoryOverhead" = "512m"
  }
}

# Lifecycle hooks configuration
hooks {
  logging {
    level = "INFO"
    format = "json"
    output = "stdout"
    structured = true
  }

  metrics {
    enabled = true
    backend = "prometheus"
    push_gateway_url = "http://localhost:9091"
    export_interval_seconds = 60
  }

  audit {
    enabled = true
    include_data_samples = false
    audit_trail_path = "/var/log/pipeline/audit"
    retention_days = 90
  }
}

# Secrets management (optional)
secrets {
  provider = "env"
  secret_prefix = "PIPELINE_"
  cache_ttl_seconds = 300
}

# Pipeline components
components = [
  # Source component
  {
    name = "customer-source"
    component_type = "source"
    class_path = "myapp.sources.CustomerSource"
    enabled = true
    config {
      input_path = "s3://data-lake/raw/customers/"
      format = "parquet"
    }
    retry {
      max_attempts = 3
      initial_delay_seconds = 1.0
      max_delay_seconds = 60.0
      backoff_multiplier = 2.0
      retry_on_exceptions = ["IOError", "TimeoutError"]
    }
  },

  # Transformation component
  {
    name = "customer-transform"
    component_type = "transformation"
    class_path = "myapp.transforms.CustomerTransform"
    enabled = true
    depends_on = ["customer-source"]
    config {
      deduplicate = true
      validate_schema = true
      enrich_data = true
    }
    retry {
      max_attempts = 5
      initial_delay_seconds = 2.0
      max_delay_seconds = 120.0
      backoff_multiplier = 2.0
      retry_on_exceptions = ["Exception"]
    }
    circuit_breaker {
      failure_threshold = 5
      success_threshold = 2
      timeout_seconds = 60.0
      half_open_max_calls = 1
    }
  },

  # Sink component
  {
    name = "customer-sink"
    component_type = "sink"
    class_path = "myapp.sinks.CustomerSink"
    enabled = true
    depends_on = ["customer-transform"]
    config {
      output_path = "s3://data-lake/processed/customers/"
      format = "parquet"
      partition_by = ["year", "month", "day"]
      mode = "overwrite"
    }
  }
]

# Arbitrary tags for metadata
tags {
  team = "data-engineering"
  project = "customer-analytics"
  cost_center = "engineering"
  owner = "data-team@company.com"
}
