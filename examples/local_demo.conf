# Local demo pipeline â€” runs entirely on local files.
#
# Reads sample customers from CSV, cleans and normalises with SQL,
# and writes the result to a temporary CSV directory.
#
# Usage:
#   python examples/run_local_demo.py

{
  name: "local-demo"
  version: "1.0.0"
  environment: dev
  mode: batch

  spark {
    app_name: "Local Demo Pipeline"
    master: "local[*]"
  }

  components: [
    {
      name: "read_customers"
      component_type: source
      class_path: "pyspark_pipeline_framework.examples.batch.ReadCsv"
      config {
        path: "examples/resources/customers.csv"
        output_view: "raw_customers"
      }
    },
    {
      name: "clean_customers"
      component_type: transformation
      class_path: "pyspark_pipeline_framework.examples.batch.SqlTransform"
      depends_on: ["read_customers"]
      config {
        sql: """
          SELECT
            customer_id,
            UPPER(name)   AS name,
            LOWER(email)  AS email,
            region,
            created_at
          FROM raw_customers
          WHERE email IS NOT NULL
        """
        output_view: "cleaned_customers"
      }
    },
    {
      name: "write_customers"
      component_type: sink
      class_path: "pyspark_pipeline_framework.examples.batch.WriteCsv"
      depends_on: ["clean_customers"]
      config {
        input_view: "cleaned_customers"
        path: "/tmp/ppf-local-demo/output"
      }
    }
  ]
}
